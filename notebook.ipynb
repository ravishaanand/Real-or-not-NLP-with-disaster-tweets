{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest= pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsub_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.duplicated().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop_duplicates().reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class balance\n# train.target.value_counts()\nsns.countplot(y=train.target);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint (train.keyword.nunique(), test.keyword.nunique())\nprint (set(train.keyword.unique()) - set(test.keyword.unique()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common keywords\nplt.figure(figsize=(10,6))\nsns.countplot(y=train.keyword, order = train.keyword.value_counts().iloc[:15].index)\nplt.title('Top 15 keywords')\nplt.show()\n# train.keyword.value_counts().head(10)\n#keywords play important role in deciding disaster tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kw_d = train[train.target==1].keyword.value_counts().head(10)\nkw_nd = train[train.target==0].keyword.value_counts().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index, color='c')\nplt.title('Top keywords for disaster tweets')\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index, color='y')\nplt.title('Top keywords for non-disaster tweets')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_d = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = train.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check number of unique keywords and locations\nprint (train.location.nunique(), test.location.nunique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common locations\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\nplt.title('Top 15 locations')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_loc = train.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = train[train.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill NA values\nfor col in ['keyword','location']:\n    train[col] = train[col].fillna('None')\n    test[col] = test[col].fillna('None')\n\ndef clean_loc(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_loc:\n        return x\n    else: return 'Others'\n    \ntrain['location_clean'] = train['location'].apply(lambda x: clean_loc(str(x)))\ntest['location_clean'] = test['location'].apply(lambda x: clean_loc(str(x)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_l2 = train.groupby('location_clean').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l2.index, y=top_l2)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ntest_str = train.loc[417, 'text']\n\ndef clean_text(text):\n    text = re.sub(r'https?://\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\n\nprint(\"Original text: \" + test_str)\nprint(\"Cleaned text: \" + clean_text(test_str))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'\n\ndef process_text(df):\n    \n    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    # df['hashtags'].fillna(value='no', inplace=True)\n    # df['mentions'].fillna(value='no', inplace=True)\n    \n    return df\n    \ntrain = process_text(train)\ntest = process_text(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import STOPWORDS\n\ndef create_stat(df):\n    # Tweet length\n    df['text_len'] = df['text_clean'].apply(len)\n    # Word count\n    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n    # Stopword count\n    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n    # Punctuation count\n    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    # Count of hashtags (#)\n    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n    # Count of mentions (@)\n    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n    # Count of links\n    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n    # Count of uppercase letters\n    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n    # Ratio of uppercase letters\n    df['caps_ratio'] = df['caps_count'] / df['text_len']\n    return df\n\ntrain = create_stat(train)\ntest = create_stat(test)\n\nprint(train.shape, test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['target'].drop('target').sort_values()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import FreqDist, word_tokenize\n\n# Make a set of stop words\nstopwords = set(STOPWORDS)\n# more_stopwords = {'https', 'amp'}\n# stopwords = stopwords.union(more_stopwords)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unigrams\nword_freq = FreqDist(w for w in word_tokenize(' '.join(train['text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['count'])\ntop20w = df_word_freq.sort_values('count',ascending=False).head(20)\n\nplt.figure(figsize=(8,6))\nsns.barplot(top20w['count'], top20w.index)\nplt.title('Top 20 words')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,7))\nplt.subplot(121)\nfreq_d = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_d = pd.DataFrame.from_dict(freq_d, orient='index', columns=['count'])\ntop20_d = df_d.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_d['count'], top20_d.index, color='c')\nplt.title('Top words in disaster tweets')\nplt.subplot(122)\nfreq_nd = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_nd = pd.DataFrame.from_dict(freq_nd, orient='index', columns=['count'])\ntop20_nd = df_nd.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_nd['count'], top20_nd.index, color='y')\nplt.title('Top words in non-disaster tweets')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bigrams\n\nfrom nltk import bigrams\n\nplt.figure(figsize=(16,7))\nplt.subplot(121)\nbigram_d = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nd_fq = FreqDist(bg for bg in bigram_d)\nbgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\nbgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\nbgdf_d = bgdf_d.sort_values('count',ascending=False)\nsns.barplot(bgdf_d.head(20)['count'], bgdf_d.index[:20], color='pink')\nplt.title('Top bigrams in disaster tweets')\nplt.subplot(122)\nbigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nnd_fq = FreqDist(bg for bg in bigram_nd)\nbgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\nbgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\nbgdf_nd = bgdf_nd.sort_values('count',ascending=False)\nsns.barplot(bgdf_nd.head(20)['count'], bgdf_nd.index[:20], color='yellow')\nplt.title('Top bigrams in non-disaster tweets')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\"):\n    \n    my_df = pd.read_csv(test_path)\n    \n    res_df = my_df[['id']]\n    my_df = my_df[['text']]\n    \n    add_hashtags(my_df)\n    my_df = clean_df(my_df)\n    print(\"Test DF: {}\".format(my_df.head(10)))\n    \n    return my_df, res_df\n\ndef dump_preds(res_df, preds, out=\"default\"):\n    res_df['target'] = None\n    \n    for i, p in  enumerate(preds):\n        res_df.ix[i, 'target'] = p\n    \n    res_df.to_csv(out, index = False)\n    \n\ndef split_data(df, _t=True):\n    X = df.text\n    if _t:\n        Y = df.target\n        le = LabelEncoder()\n        Y = le.fit_transform(Y)\n        Y = Y.reshape(-1,1)\n        return X, Y\n    else:\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\n\"\"\"\nBuild a baseline TFIDF + LOGREG based just on text\n\"\"\"\ndef build_tfidf_logreg(df):\n    my_df = df[['text','target']]\n    x_features = my_df.columns[0]\n    x_data = my_df[x_features]\n    Y = my_df[\"target\"]\n\n    x_train, x_validation, y_train, y_validation = model_selection.train_test_split(\n        x_data.values, Y.values, test_size=0.2, random_state=7)\n    \n    # configure TfidfVectorizer to accept tokenized data\n    # reference http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n    tfidf_vectorizer = TfidfVectorizer(\n        analyzer='word',\n        tokenizer=lambda x: x,\n        preprocessor=lambda x: x,\n        token_pattern=None)\n\n    lr = LogisticRegression()\n    tfidf_lr_pipe = Pipeline([('tfidf', tfidf_vectorizer), ('lr', lr)])\n    tfidf_lr_pipe.fit(x_train, y_train)\n    \n    return tfidf_lr_pipe\n\ndef test_tfidf_logreg(model, test_path=\"/kaggle/input/nlp-getting-started/test.csv\"):\n    \n    my_df, res_df = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n    \n    #x_features = my_df.columns[0]\n    x_data = my_df[\"text\"].values\n\n    preds = model.predict(x_data)\n    \n    #dump_preds(res_df, preds, out=\"res_tfidf_logreg4_0.csv\")\n    \n    return res_df\n\n\n\"\"\"\nBuild a majority model\n\"\"\"\ndef test_majority_model(test_path=\"/kaggle/input/nlp-getting-started/test.csv\"):\n    \n    my_df = pd.read_csv(test_path)\n    \n    res = my_df[['id']]\n    res['target'] = 1\n    \n    res.to_csv(\"res_majority.csv\", index = False)\n    return res\n    \n\n#test_majority_model(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n#0.42944\n\n#tfidf_log_reg = build_tfidf_logreg(df)\n#test_tfidf_logreg(tfidf_log_reg, test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n#0.63164\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\nX_train, y_train = split_data(train)\n\ntest_df, res_df = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\nX_test = split_data(test_df, _t=False)\n\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', GradientBoostingClassifier(n_estimators=100)),\n                     ])\n#text_clf.fit(X_train, y_train)\n#predicted = text_clf.predict(X_test)\n#dump_preds(res_df, predicted, out=\"submission.csv\")\n#0.66462\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\n# Target encoding\nfeatures = ['keyword', 'location_clean']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'))\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# CountVectorizer\n\n# Links\nvec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?://\\S+') # Only include those >=5 occurrences\nlink_vec = vec_links.fit_transform(train['links'])\nlink_vec_test = vec_links.transform(test['links'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n\n# Mentions\nvec_men = CountVectorizer(min_df = 5)\nmen_vec = vec_men.fit_transform(train['mentions'])\nmen_vec_test = vec_men.transform(test['mentions'])\nX_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\nX_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())\n\n# Hashtags\nvec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train['hashtags'])\nhash_vec_test = vec_hash.transform(test['hashtags'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())\nprint (X_train_link.shape, X_train_men.shape, X_train_hash.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = (X_train_link.transpose().dot(train['target']) / X_train_link.sum(axis=0)).sort_values(ascending=False)\nplt.figure(figsize=(10,6))\nsns.barplot(x=_, y=_.index)\nplt.axvline(np.mean(train.target))\nplt.title('% of disaster tweet given links')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = (X_train_men.transpose().dot(train['target']) / X_train_men.sum(axis=0)).sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=_.index, y=_)\nplt.axhline(np.mean(train.target))\nplt.title('% of disaster tweet given mentions')\nplt.xticks(rotation = 50)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_rank = (X_train_hash.transpose().dot(train['target']) / X_train_hash.sum(axis=0)).sort_values(ascending=False)\nprint('Hashtags with which 100% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==1].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==1])))\nprint('Hashtags with which 0% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==0].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==0])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tf-idf for text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n# Only include >=10 occurrences\n# Have unigrams and bigrams\ntext_vec = vec_text.fit_transform(train['text_clean'])\ntext_vec_test = vec_text.transform(test['text_clean'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\nprint (X_train_text.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joining the dataframes together\n\ntrain = train.join(X_train_link, rsuffix='_link')\ntrain = train.join(X_train_men, rsuffix='_mention')\ntrain = train.join(X_train_hash, rsuffix='_hashtag')\ntrain = train.join(X_train_text, rsuffix='_text')\ntest = test.join(X_test_link, rsuffix='_link')\ntest = test.join(X_test_men, rsuffix='_mention')\ntest = test.join(X_test_hash, rsuffix='_hashtag')\ntest = test.join(X_test_text, rsuffix='_text')\nprint (train.shape, test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nfeatures_to_drop = ['id', 'keyword','location','text','location_clean','text_clean', 'hashtags', 'mentions','links']\nscaler = MinMaxScaler()\n\nX_train = train.drop(columns = features_to_drop + ['target'])\nX_test = test.drop(columns = features_to_drop)\ny_train = train.target\n\nlr = LogisticRegression(solver='liblinear', random_state=777) # Other solvers have failure to converge problem\n\npipeline = Pipeline([('scale',scaler), ('lr', lr),])\n\npipeline.fit(X_train, y_train)\ny_test = pipeline.predict(X_test)\n\nsubmit = sub_sample.copy()\nsubmit.target = y_test\nsubmit.to_csv('submit_lr.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Training accuracy: %.4f' % pipeline.score(X_train, y_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# F-1 score\nfrom sklearn.metrics import f1_score\n\nprint ('Training f-1 score: %.4f' % f1_score(y_train, pipeline.predict(X_train)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\npd.DataFrame(confusion_matrix(y_train, pipeline.predict(X_train)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\ncv_score = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top features\nplt.figure(figsize=(16,7))\ns1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values(ascending=False)[:20]\ns2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values()[:20]\nplt.subplot(121)\nsns.barplot(y=s1.index, x=s1)\nplt.title('Top positive coefficients')\nplt.subplot(122)\nsns.barplot(y=s2.index, x=s2)\nplt.title('Top negative coefficients')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature selection\nfrom sklearn.feature_selection import RFECV\n\nsteps = 20\nn_features = len(X_train.columns)\nX_range = np.arange(n_features - (int(n_features/steps)) * steps, n_features+1, steps)\n\nrfecv = RFECV(estimator=lr, step=steps, cv=cv, scoring='f1')\n\npipeline2 = Pipeline([('scale',scaler), ('rfecv', rfecv)])\npipeline2.fit(X_train, y_train)\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(np.insert(X_range, 0, 1), rfecv.grid_scores_)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Optimal no. of features: %d' % np.insert(X_range, 0, 1)[np.argmax(rfecv.grid_scores_)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = X_train.columns[rfecv.ranking_ == 1]\nX_train2 = X_train[selected_features]\nX_test2 = X_test[selected_features]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr2 = LogisticRegression(solver='liblinear', random_state=37)\npipeline.fit(X_train2, y_train)\ncv2 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=456)\ncv_score2 = cross_val_score(pipeline, X_train2, y_train, cv=cv2, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nlr_cv = GridSearchCV(LogisticRegression(solver='liblinear', random_state=20), grid, cv=cv2, scoring = 'f1')\n\npipeline_grid = Pipeline([('scale',scaler), ('gridsearch', lr_cv),])\n\npipeline_grid.fit(X_train2, y_train)\n\nprint(\"Best parameter: \", lr_cv.best_params_)\nprint(\"F-1 score: %.3f\" %lr_cv.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submit fine-tuned model\n\ny_test2 = pipeline_grid.predict(X_test2)\nsubmit2 = sub_sample.copy()\nsubmit2.target = y_test2\nsubmit2.to_csv('submit_lr2.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top features with fine-tuned model\nplt.figure(figsize=(16,7))\ns1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values(ascending=False)[:20]\ns2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values()[:20]\nplt.subplot(121)\nsns.barplot(y=s1.index, x=s1)\nplt.title('Top positive coefficients')\nplt.subplot(122)\nsns.barplot(y=s2.index, x=s2)\nplt.title('Top negative coefficients')\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}